{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"1RJAOCC1y_lzpRXsWVRCPpYIVovRKFq4s","timestamp":1726839084838},{"file_id":"1_Y_YWhqRUJ52uPdcz-0jaYl3TfsStbos","timestamp":1726837046228}],"authorship_tag":"ABX9TyMuUVfjkvjhjC4+nDZVmU3c"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["*  整理自 https://github.com/ZhiqingXiao/rl-book/blob/master/en2024/code/CartPole-v0_OffPolicyVPGwBaseline_torch.ipynb\n","\n","\n","<mark>指定行为策略 $b$</mark>\n","\n"],"metadata":{"id":"K2XzEEs1B7wf"}},{"cell_type":"code","source":["!pip install gymnasium"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"6dc3a0OUD6aq","executionInfo":{"status":"ok","timestamp":1727079624995,"user_tz":-480,"elapsed":12953,"user":{"displayName":"Helmi Standerwick","userId":"01404479694440072705"}},"outputId":"62ef0fcd-1cca-428b-8597-b5b38f34fe8e"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting gymnasium\n","  Downloading gymnasium-0.29.1-py3-none-any.whl.metadata (10 kB)\n","Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium) (1.26.4)\n","Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium) (2.2.1)\n","Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium) (4.12.2)\n","Collecting farama-notifications>=0.0.1 (from gymnasium)\n","  Downloading Farama_Notifications-0.0.4-py3-none-any.whl.metadata (558 bytes)\n","Downloading gymnasium-0.29.1-py3-none-any.whl (953 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m953.9/953.9 kB\u001b[0m \u001b[31m13.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading Farama_Notifications-0.0.4-py3-none-any.whl (2.5 kB)\n","Installing collected packages: farama-notifications, gymnasium\n","Successfully installed farama-notifications-0.0.4 gymnasium-0.29.1\n"]}]},{"cell_type":"code","execution_count":2,"metadata":{"id":"xaGangygB7Eb","executionInfo":{"status":"ok","timestamp":1727079636927,"user_tz":-480,"elapsed":11934,"user":{"displayName":"Helmi Standerwick","userId":"01404479694440072705"}}},"outputs":[],"source":["%matplotlib inline\n","\n","import sys\n","import logging\n","import itertools  # 用于构建不限制次数的循环\n","\n","import numpy as np\n","np.random.seed(0)\n","import pandas as pd\n","import gymnasium as gym\n","import matplotlib.pyplot as plt\n","import torch\n","torch.manual_seed(0)\n","import torch.nn as nn\n","import torch.optim as optim\n","import torch.distributions as distributions\n","\n","logging.basicConfig(level=logging.INFO,\n","          format='%(asctime)s [%(levelname)s]%(message)s',\n","          stream=sys.stdout, datefmt='%H；%M:%S')\n","\n","import warnings   # 忽略警告\n","warnings.filterwarnings(\"ignore\")"]},{"cell_type":"markdown","source":["## 环境\n","\n","\n","* https://gymnasium.farama.org/environments/classic_control/cart_pole/"],"metadata":{"id":"3opYAxHGFlJO"}},{"cell_type":"code","source":["env = gym.make('CartPole-v1', render_mode='rgb_array')  # 奖励阈值 500\n","print(env.action_space.n) # 动作数"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"sqXDTH56D5ea","executionInfo":{"status":"ok","timestamp":1727079636928,"user_tz":-480,"elapsed":5,"user":{"displayName":"Helmi Standerwick","userId":"01404479694440072705"}},"outputId":"1b286ab7-93ea-477a-cb1b-e912b62666f6"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["2\n"]}]},{"cell_type":"markdown","source":["## 算法 模块"],"metadata":{"id":"Dz8JvGWOGvwW"}},{"cell_type":"code","source":["class REINFORCEAgent:\n","  def __init__(self, env):\n","    self.action_n = env.action_space.n\n","    self.gamma = 0.99 # 折扣因子\n","\n","    self.policy_net = self.build_net(\n","        input_size = env.observation_space.shape[0],\n","        hidden_sizes = [],\n","        output_size = self.action_n, output_activator=nn.Softmax(1))  # nn.Softmax(dim=None)： dim=0表示对列进行归一化，dim=1表示对行进行归一化\n","    self.policy_optimizer = optim.Adam(self.policy_net.parameters(), lr=0.06) # 学习率调大了\n","\n","    ### 对 基线的估计 网络\n","    self.baseline_net = self.build_net(\n","        input_size = env.observation_space.shape[0],\n","        hidden_sizes = [])\n","    self.baseline_optimizer = optim.Adam(self.policy_net.parameters(), lr=0.1) # 学习率调大了\n","    self.baseline_loss = nn.MSELoss()\n","\n","\n","  def build_net(self, input_size, hidden_sizes, output_size=1,\n","          output_activator=None, use_bias=False):\n","    layers = []\n","    for input_size, output_size in zip([input_size,]+hidden_sizes, hidden_sizes+[output_size,]):\n","      layers.append(nn.Linear(input_size, output_size, bias=use_bias))\n","      layers.append(nn.ReLU())\n","    layers = layers[:-1]\n","    if output_activator:\n","      layers.append(output_activator)\n","    model = nn.Sequential(*layers)\n","    return model\n","\n","  def reset(self, mode=None):\n","    self.mode = mode\n","    if self.mode == \"train\":\n","      self.trajectory = []\n","\n","  def step(self, observation, reward, terminated): # 修改\n","\n","    if self.mode == \"train\":  #\n","      action = np.random.choice(self.action_n) # 使用随机策略\n","      self.trajectory += [observation, reward, terminated, action]\n","    else:\n","      state_tensor = torch.as_tensor(observation, dtype=torch.float).unsqueeze(0)\n","      prob_tensor = self.policy_net(state_tensor)  #\n","      action = distributions.Categorical(prob_tensor).sample().item()\n","\n","    return action\n","\n","  def close(self):\n","    if self.mode == \"train\":\n","      self.learn()\n","\n","  def learn(self):  # 学习 模块\n","    state_tensor = torch.as_tensor(self.trajectory[0::4], dtype=torch.float)\n","    reward_tensor = torch.as_tensor(self.trajectory[1::4], dtype=torch.float)\n","    action_tensor = torch.as_tensor(self.trajectory[3::4], dtype=torch.long)\n","    arange_tensor = torch.arange(state_tensor.shape[0],dtype=torch.float)\n","\n","    #\n","    discount_tensor = self.gamma ** arange_tensor\n","    discounted_reward_tensor = discount_tensor * reward_tensor\n","    discounted_return_tensor = discounted_reward_tensor.flip(0).cumsum(0).flip(0)\n","    # 更新 baseline 网络的 参数\n","    return_tensor = discounted_return_tensor / discount_tensor\n","    pred_tensor = self.baseline_net(state_tensor)\n","    psi_tensor = (discounted_return_tensor - discount_tensor * pred_tensor).detach()\n","    baseline_loss_tensor = self.baseline_loss(pred_tensor,return_tensor.unsqueeze(1))\n","    self.baseline_optimizer.zero_grad()\n","    baseline_loss_tensor.backward()\n","    self.baseline_optimizer.step()\n","\n","    # # 更新 策略网络的 参数\n","    all_pi_tensor = self.policy_net(state_tensor)\n","    pi_tensor = torch.gather(all_pi_tensor, 1, action_tensor.unsqueeze(1)).squeeze(1)\n","\n","    # 行为网络参数\n","    behavior_prob = 1. / self.action_n\n","    policy_loss_tensor = -(psi_tensor / behavior_prob * pi_tensor).mean() #\n","\n","    self.policy_optimizer.zero_grad() # 重置 梯度\n","    policy_loss_tensor.backward() # 反向传播\n","    self.policy_optimizer.step() # 更新权重\n","\n","agent = REINFORCEAgent(env)"],"metadata":{"id":"ZccYBq1JGymR","executionInfo":{"status":"ok","timestamp":1727079638945,"user_tz":-480,"elapsed":2021,"user":{"displayName":"Helmi Standerwick","userId":"01404479694440072705"}}},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":["## 训练"],"metadata":{"id":"qF_FSgh7JdQ5"}},{"cell_type":"code","source":["from IPython import display\n","\n","def play_episode(env, agent, seed=None, mode=None, render=False):\n","  observation, _ = env.reset(seed=seed)\n","  reward, terminated, truncated = 0., False, False\n","  agent.reset(mode=mode)\n","  episode_reward, elapsed_steps = 0., 0\n","\n","  if not mode and episode == 99:  # 只可视化最后一个 episode\n","    img = plt.imshow(env.render()) # only call this once  # 可视化\n","\n","  while True:\n","\n","    # Jupyter notebook  可视化 测试结果  仅显示 最后 3 个 回合\n","    if not mode and episode == 99:\n","        img.set_data(env.render()) # just update the data\n","        display.display(plt.gcf())\n","        display.clear_output(wait=True)\n","\n","    action = agent.step(observation, reward, terminated)\n","    observation, reward, terminated, truncated, info = env.step(action)\n","    if render:\n","      env.render()\n","    if terminated or truncated:\n","      break\n","    episode_reward += reward\n","    elapsed_steps += 1 # 走过的步数\n","  agent.close()\n","\n","  return episode_reward, elapsed_steps\n","\n","logging.info(\"===== train =====\")\n","episode_rewards = []\n","for episode in itertools.count(): # 从 0 到无穷大的 迭代次数\n","  episode_reward, elapsed_steps = play_episode(env, agent, seed=episode, mode=\"train\")\n","  episode_rewards.append(episode_reward)\n","  logging.info('train episode %d：reward = %.2f, steps = %d',\n","               episode, episode_reward, elapsed_steps)\n","  if np.mean(episode_rewards[-20:]) > 200: # 获得有效的策略了  可修改得更大些，学习更好的策略，同时学习时间会变长\n","    break\n","\n","plt.plot(episode_rewards)\n","\n","logging.info(\"===== test =====\")\n","episode_rewards_test = []\n","for episode in range(100):\n","  epsiode_reward, elapsed_steps = play_episode(env, agent)\n","  episode_rewards_test.append(episode_reward)\n","  logging.info('test episode % d: reward = %.2f, steps=%d',\n","        episode, episode_reward, elapsed_steps)\n","logging.info(\"average episode reward = %.2f ± %.2f\",\n","       np.mean(episode_rewards_test), np.std(episode_rewards_test))\n","\n","env.close()"],"metadata":{"id":"7RC8I1wiZZBw"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 平滑\n","\n","def moving_average(a, window_size):\n","    cumulative_sum = np.cumsum(np.insert(a, 0, 0))\n","    middle = (cumulative_sum[window_size:] - cumulative_sum[:-window_size]) / window_size\n","    r = np.arange(1, window_size-1, 2)\n","    begin = np.cumsum(a[:window_size-1])[::2] / r\n","    end = (np.cumsum(a[:-window_size:-1])[::2] / r)[::-1]\n","    return np.concatenate((begin, middle, end))\n","\n","episodes_list = list(range(len(episode_rewards)))  # x\n","mv_return = moving_average(episode_rewards, 29)  # 9 可考虑改大些，让图好看些\n","plt.plot(episodes_list, mv_return)  # x y\n","plt.xlabel('Episodes')\n","plt.ylabel('Returns')\n","plt.title('REINFORCE on {}'.format(env_name))\n","plt.show()"],"metadata":{"id":"lueqw9T9HxUH"},"execution_count":null,"outputs":[]}]}